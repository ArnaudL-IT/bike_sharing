{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import local packages\n",
    "from src.data_processing import load_csv_from_zip as lcfz\n",
    "from src.data_processing import data_preprocessing as dpp\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = lcfz.read_csv_from_zip('./../../data/input/bike-sharing-demand.zip', ['train.csv'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dpp.basic_prep_wrapper(train, ['temp'])\n",
    "train = dpp.target_to_log(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = train.columns.drop(['casual', 'registered', 'count'])\n",
    "label = ['casual', 'registered', 'count']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train[features], train[label], test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning of 3 models (casual, registered, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparam_tuning(X, y, target):\n",
    "    model = RandomForestRegressor(n_jobs=-1)\n",
    "    \n",
    "    param_grid = [\n",
    "        {\n",
    "            \"n_estimators\": [900],  #range(100, 1200, 100),\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    gs = GridSearchCV(model, param_grid, scoring=\"neg_mean_squared_error\", n_jobs=-1, verbose=1)\n",
    "    gs.fit(X, y[target].values.ravel())\n",
    "    \n",
    "    cvres = gs.cv_results_\n",
    "    \n",
    "    print(\"Best estimator is :\\n\")\n",
    "    print(gs.best_estimator_)\n",
    "    print(\"KFoldCV best score = {}\".format(gs.best_score_))\n",
    "    \n",
    "    return gs.best_estimator_, cvres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_performances(results):\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.plot(x='param_n_estimators',y='mean_test_score')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain_model(model, X, y, target):\n",
    "    model.fit(X, y[target].values.ravel())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_and_train(X, y, target):\n",
    "    model, results = hyperparam_tuning(X, y, target)\n",
    "    model_performances(results)\n",
    "    model = retrain_model(model, X, y, target)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mdl_casual = optimize_and_train(X_train, y_train, 'casual')\n",
    "mdl_registered = optimize_and_train(X_train, y_train, 'registered')\n",
    "mdl_count = optimize_and_train(X_train, y_train, 'count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study model performances on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(model, X):\n",
    "    predictions = np.expm1(model.predict(X))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_results(casual_preds, registered_preds, count_preds, y):\n",
    "    results = np.expm1(y.copy())\n",
    "    results['casual_preds'], results['registered_preds'], results['count_preds'] = casual_preds, registered_preds, count_preds\n",
    "    results[results<0] = 0\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_perfs(df):\n",
    "    for target in ['casual', 'registered', 'count']:\n",
    "        preds = target+'_preds'\n",
    "        rmsle = np.sqrt(mean_squared_log_error(df[target], df[preds]))\n",
    "        rmse = np.sqrt(mean_squared_error(df[target], df[preds]))\n",
    "        print(\"Target is : {}\".format(target))\n",
    "        print(\"RMSLE = {}, \\t RMSE = {}\".format(rmsle, rmse))\n",
    "        plot_results(df, target)\n",
    "        \n",
    "    rmsle = np.sqrt(mean_squared_log_error(df['count'], df['casual_preds']+df['registered_preds']))\n",
    "    rmse = np.sqrt(mean_squared_error(df['count'], df['casual_preds']+df['registered_preds']))\n",
    "    print(\"Target is : {}\".format(\"composite count\"))\n",
    "    print(\"RMSLE = {}, \\t RMSE = {}\".format(rmsle, rmse))\n",
    "    \n",
    "    target = 'composite'\n",
    "    plot_results(df, target)\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(df, target):\n",
    "    # Sort the dataframe by datetimeindex for coherent time serie plots\n",
    "    df.sort_index(inplace=True)\n",
    "\n",
    "    if target == 'composite':\n",
    "        y_preds = df['casual_preds']+df['registered_preds']\n",
    "        y = df['count']\n",
    "    else:\n",
    "        y_preds = df[target+'_preds']\n",
    "        y = df[target]\n",
    "    \n",
    "    fig, ax = plt.subplots(2, 2, figsize=(12,9))\n",
    "    \n",
    "    # Plot the time series of predictions and actual values\n",
    "    ax[0][0].plot(df.index, y, color='g', alpha=0.6)\n",
    "    ax[0][0].plot(df.index, y_preds, color='r', alpha=0.6)\n",
    "    \n",
    "    # Plot the predictions versus the actual values as a scatter plot\n",
    "    ax[0][1].plot(y, y_preds, marker='o', linewidth=0, alpha=0.6)\n",
    "    ax[0][1].plot(range(800), range(800), 'r-')\n",
    "    \n",
    "    # Plot the residuals as a time serie\n",
    "    ax[1][0].plot(df.index, y_preds-y, color='r', alpha=0.6)\n",
    "    ax[1][0].plot(df.index, [0 for _ in range(len(df))], 'b-')\n",
    "\n",
    "    # Plot the histogram of the residuals\n",
    "    ax[1][1].hist(x=(y_preds-y), bins=50)\n",
    "    #ax[1][1].plot(x=[0 for _ in range(2)], y=[0, 600], 'r-')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import mpld3\n",
    "mpld3.enable_notebook()\n",
    "\n",
    "casual_preds = make_predictions(mdl_casual, X_test)\n",
    "registered_preds = make_predictions(mdl_registered, X_test)\n",
    "count_preds = make_predictions(mdl_count, X_test)\n",
    "\n",
    "results = format_results(casual_preds, registered_preds, count_preds, y_test)\n",
    "\n",
    "summarize_perfs(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, sharey=True, figsize=(12,9))\n",
    "\n",
    "ax[0].barh(np.arange(len(features)), mdl_casual.feature_importances_)\n",
    "ax[1].barh(np.arange(len(features)), mdl_registered.feature_importances_)\n",
    "ax[2].barh(np.arange(len(features)), mdl_count.feature_importances_)\n",
    "ax[0].set_yticks(np.arange(len(features)))\n",
    "ax[0].set_yticklabels(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain the model on the entire train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_casual = mdl_casual.fit(train[features], train['casual'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(mdl_casual, open('./../../models/trained/RandomForest_casual.sav', 'wb'))\n",
    "pickle.dump(mdl_registered, open('./../../models/trained/RandomForest_registered.sav', 'wb'))\n",
    "pickle.dump(mdl_count, open('./../../models/trained/RandomForest_count.sav', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
